<DOCTYPE! html>
    <html>

    <head>
        <title>AI & Oppression</title>
        <link href="styles.css" rel="stylesheet" type="text/css" />
    </head>

    <body>
        <header>
            <h1>OUR TECHNOLOGICAL FUTURE</h1>
            <h2>AI, BIAS & THE THREAT TO DEMOCRACY</h2>
            <nav>
                <ul>
                    <li><a href="index.html">HOME</a></li>
                    <li><a href="resources.html">RESOURCES</a></li>
                </ul>
            </nav>
        </header>
        <div class="padlet-embed" id="algsOppPad" style="border:1px solid rgba(0,0,0,0.1);border-radius:2px;box-sizing:border-box;overflow:hidden;position:relative;width:100%;background:#F4F4F4">
            <p style="padding:0;margin:0"><iframe src="https://padlet.com/embed/g7errzo4kffboybz" scrolling="no" style="width:100%;height:500px;overflow:hidden"></iframe></p>
        </div>
        <div id="main">
            <div id="">
                <h1>How Does AI Perpetuate Oppression?</h1>
                <!-- <h2>....</h2> -->
                <br/>
                <blockquote>"We have more data and technology than ever in our daily lives and more social, political, and economic inequality and injustice to go with it." -Safiya Noble, author of Algorithms of Oppression
                </blockquote>
                <br/>
                <p>In understanding how algorithms further oppression, one must first understand that algorithms are not neutral because human beings are not neutral. Each one of us is biased, whether we like to admit it or not. We've been shaped by our
                    life experiences, and therefore hold implicit biases. These biases are naturally encoded into the machines we create, thus reflecting the world as we know it. They are built from data, utilizing the past to construct the future while
                    failing to recognize our past is flawed with deeply rooted oppressive patterns. Algorithms hold the power to both mask history and repeat it, while simultaneously strengthening the digital divide and altering the marketplace of ideas.</p>
                <br/>
                <p>Looking at how artificial intelligence is developed, Safiya Noble argues that "it reflects the same biases as the culture that creates it" (Noble 140). We are not exempt from implicit bias, so it’s crucial that we understand the algorithms
                    aren't either. We also must take a closer look at who is creating these behind the scenes, because we will find a specific demographic. Less than 14% of AI researchers are women (<i>Coded Bias</i>), and as of July 1, 2016, Google's
                    workforce consisted of only 2% African American and 3% Latinx workers (Noble 163). This is where we get the coded gaze, or the harmful bias encoded in AI, which stems from the white male gaze as they are primarily the ones creating
                    these algorithms. It's from here that we can begin to understand how these biases fabricate technological redlining to advance advertising profits and benefit those who hold the power while further oppressing those who don't.
                </p>
                <br/>
                <p>According to Shalini Kantayya, "AI is increasingly becoming a gatekeeper of opportunity" as it decides things like who gets a job, who has access to healthcare and what quality they will receive, what communities face police scrutiny,
                    how long of a prison sentence someone will serve, what kind of housing someone can get, and the list goes on and on. Once again, no one is exempt from this - our opportunities are now being screened and decided by a machine. Not only
                    this, but we are unable to see how it arrives at the conclusions that it does, raising the question, how can we make the invisible visible? Pushing for transparency is a great place to start, but that alone will not solve the ever-growing
                    digital divide. With bias encoded into AI, unequal opportunities are placing barriers on our access to everyday needs.
                </p>
                <br/>
                <!-- <blockquote>"Media stereotypes, which include search engine results, not only mask the unequal access to social, political and economic life in the United States as broken down by race, gender, and sexuality; they maintain it." -Safiya Noble
                </blockquote> -->
                <blockquote>"The point is not whether some people benefit. It's that so many suffer. These models, powered by algorithms, slam doors in the face of millions of people, often for the flimsiest of reasons, and offer no appeal. They're unfair." <br/>-
                    Cathy O'Neil, author of Weapons of Math Destruction</blockquote>
                <br/>
                <p>While we think of AI as futuristic, it's developed based upon data so it's actually built from the past, forcing us to closely examine our history. We live in a socially constructed society built on prioritizing one group over another,
                    one race over another, one gender over another, so we can see how these are enveloped into our technology, once again reflecting the homogenous group who designed it. Additionally, in the past, we have relied on educators, librarians,
                    books and experiences to expand our knowledge base and provide answers to our questions. Now, we rely on Google. While this is problematic for many reasons, the main misconception is that we think of Google as an encyclopedia, not
                    an advertising company. Unlike libraries and other educational platforms, Google does not prioritize fact checking and we don’t think twice about it. Google, and other search engines, hold the power to mask or rewrite history, and
                    we would have no way of knowing the difference without deeply researching ourselves, which the majority of the population would not take the time to do. This reframes the marketplace of ideas as AI chooses and limits what ideas we
                    have access to. Adding this layer to the complexity of bias in AI leads to further oppression of marginalized groups. Noble points out how "Silicon Valley executives revel in their embrace of colorblindness as if it is an asset and
                    not a proven liability" (Noble 168) meaning they ignore marginalization and their role in it. These companies and executives place responsibility on the individual, rather than acknowledging that the algorithms they have created are
                    truly at fault for perpetuating this oppression.
                </p>
                <br/>
                <p>When looking at AI and the role it plays in furthering oppression, we see how bias is encoded into the algorithms that run essential systems in our daily lives. Algorithms have the power to mask our history and repeat it, while limiting
                    us in our access to knowledge and opportunity. We desperately need transparency and regulation to combat this power system that favors those who have designed it.
                </p>
                <br/>
                <br/>
                <div class="conWorks">
                    <h2>CONSULTED WORKS</h2>
                    <ul>
                        <li><a href="https://mitsloan.mit.edu/ideas-made-to-matter/3-ways-to-make-technology-more-equitable" target="_blank">Brown, S. (2020, Nov 17). <i>3 Ways to Make Technology More Equitable</i>. MIT Management Sloan School.</a></li>
                        <li><a href="https://open.spotify.com/episode/4fWy9XecTCgN1pzJUpYjp9?si=6vsuwOIpQiW6mrdpJKAIxg&dl_branch=1" target="_blank">Harris, T., & Raskin, A. (Hosts). (2021, April 8). Coded Bias [Audio podcast episode]. In <i>Your Undivided Attention</i>. The Center For Humane Technology.</a></li>
                        <li><a href="https://www.netflix.com/title/81328723" target="_blank">Kantayya, S. (Director). (2020). <i>Coded Bias </i> [Film]. 7th Empire Media.</a></li>
                        <li>Noble, S.U. (2018). <i>Algorithms of Oppression: How Search Engines Reinforce Racism. New York University Press.</i></li>
                        <li><a href="https://www.brookings.edu/wp-content/uploads/2020/08/Simons-Ghosh_Utilities-for-Democracy_PDF.pdf" target="_blank">Simons, J. & Ghosh, D. (2020, Aug). <i>Utilities for Democracy: Why and How the Algorithmic Infrastructure of Facebook and Google Must Be Regulated</i>. Brookings Institute.</a></li>
                    </ul>
                    <br/>
                    <br/>
                </div>
            </div>

            <div class="recRes">
                <h3>RECOMMENDED RESOURCES</h3>
                <ul>
                    <li>Benjamin, R. (2019). <i>Race After Technology: Abolitionist Tools for the New Jim Crow</i>. Polity Press.
                    </li>
                    <li><a href="https://www.all-turtles.com/podcast/series/culture-fit" target="_blank">Boyd, D. (2020). Culture Fit: Racial Bias in Tech [Audio podcast]. All Turtles Podcast.</a></li>
                    <li><a href="https://mitsloan.mit.edu/ideas-made-to-matter/3-ways-to-make-technology-more-equitable" target="_blank">Brown, S. (2020, Nov 17). <i>3 Ways to Make Technology More Equitable</i>. MIT Management Sloan School.</a></li>
                    <li><a href="https://doi.org/10.1145/3342194" target="_blank">Gastil, J. & Davies, T. (2019). <i>Digital Democracy: Episode IV—A New Hope: How a Corporation for Public Software Could Transform Digital Engagement for Government and Civil Society.</i></a></li>
                    <li><a href="https://open.spotify.com/episode/4fWy9XecTCgN1pzJUpYjp9?si=6vsuwOIpQiW6mrdpJKAIxg&dl_branch=1" target="_blank">Harris, T., & Raskin, A. (Hosts). (2021, April 8). Coded Bias [Audio podcast episode]. In <i>Your Undivided Attention</i>. The Center For Humane Technology.</a></li>
                    <li><a href="https://www.netflix.com/title/81328723" target="_blank">Kantayya, S. (Director). (2020). <i>Coded Bias </i> [Film]. 7th Empire Media.</a></li>
                    <li>Noble, S.U. (2018). <i>Algorithms of Oppression: How Search Engines Reinforce Racism. New York University Press.</i></li>
                    <li>O’Neil, C. (2016). <i>Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy</i>. Broadway Books.
                    </li>
                </ul>
            </div>

            <!-- <div class="conWorks">
                <h3>Consulted Works</h3>
                <ul>
                    <li>...</li>
                </ul>
            </div> -->
        </div>
        <footer>
            <ul>
                <li><a href="index.html">HOME</a></li>
                <li><a href="resources.html">RESOURCES</a></li>
            </ul>
        </footer>
    </body>

    </html>
    </DOCTYPE>